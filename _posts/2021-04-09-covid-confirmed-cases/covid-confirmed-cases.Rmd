---
title: "Exploring the Temporal Evolution of COVID-19 Cases"
description: |
  In this assignment, the purpose is to investigate the spread of Coronavirus (COVID-19) over time for different countries and building models to forecast the near-future case load in each country, using the Johns Hopkins COVID-19 dataset.
author:
  - name: Siow Mun Chai
    url: {}
date: 04-09-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview
The purpose of this assignment is to provides estimates of COVID-19 near-future case load in each country, based on global data collated by Johns Hopkins University. Beside allowing users to visualise the forecasted result, the users are also allowed to visualise the time-series data (EDA).


Before we proceed to do the forecasting models, we will first do EDA on the data. We should allow the users to visualise:
1) Distribution of the data 
2) Trend, Seasonality and correlation of the data
3) Any Anomalies in the data
4) stationarity of the data

All these factors will affect the outcome of the forecasting models.

# Data Source
The COVID-19 dataset is a timeseries dataset (with observations recorded daily). The dataset used is sourced from the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University. CSSE had collated the location and number of confirmed COVID-19 cases, deaths, and recoveries for all affected countries. It was developed to provide researchers, public health authorities, and the general public with a user-friendly tool to track the outbreak as it unfolds. The data used for this assignment is the cumulative confirmed daily cases for all affected countries and can be downloaded at https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv.


## Installing and launching R packages
A list of packages are required for this assignment. This code chunk installs the required packages and loads them onto RStudio environment.

```{r message=FALSE, warning=FALSE}

packages = c('xgboost','tidymodels','modeltime','tidyverse','lubridate','timetk','earth','tseries','forecast',
             'viridis','plotly','tidyverse','tsibble','tsintermittent','randomForest',
             'fpp3','Metrics','gghighlight')



for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

```

## Loading the dataset onto R
Data import was accomplished using read_csv() from readr package.


```{r message=FALSE, warning=FALSE}
confirmed_cases_raw <- read_csv("data/time_series_covid19_confirmed_global.csv") 
confirmed_cases_raw

```

The following are observed from the dataset:

* The date are recorded as variables and number of cases are recorded as observations, there is a need to collect all the date variables into a single date variable. 

* The date variable is currently in "double" format. Need to recode into Date format.

* The dataset contained cases at
  - the province level in China 
  - the city level in the USA, Australia, and Canada 
  - the country level otherwise
there is a need to amalgamate the province/state, city level into Country/Region.

# Data Cleaning

The following code helps to convert the data to the correct format for subsequently analysis.

* confirmed_cases_country_level to store cumulative confirmed cases for the country (amalgamating the province/state, city level)

* We are interested in the daily new confirmed cases, using lag function to do.

* Selecting only Country, Date, TotalCases and daily_new_cases to be stored.


```{r message=FALSE, warning=FALSE }
confirmed_cases_country_level <- confirmed_cases_raw %>%
  gather(Date,Total_Cases,-'Province/State',-'Country/Region',-Lat,-Long) %>%    #collecting all the date variables into a single date variable.
  group_by(`Country/Region`,Date) %>%
  summarize(total = sum(Total_Cases)) %>% 
     select(`Country/Region`, Date, total) %>%
     set_names(c("Country", "Date", "TotalCases")) %>%   #rename the variables
  ungroup()

confirmed_cases_country_level$Date <- as.Date(confirmed_cases_country_level$Date,format="%m/%d/%y")  #converting the Date variable to Date format

# finding the daily new confirmed cases.
confirmed_cases_country_level <- confirmed_cases_country_level %>%
  group_by(Country) %>%
  arrange(Country,Date)%>%
  mutate(Daily_new_cases = TotalCases - lag(TotalCases, n =1))%>%
  drop_na()

confirmed_cases_country_level

#tk_summary_diagnostics(confirmed_cases_country_level)
 
```


# EDA - Visualise the distribution of the daily confirmed cases for each country - Using TimeTK package. 

To allow users to explore / visualise the time series data for the countries, time series charts would be plotted. The following section help to explore what is the optimal number of charts that can be displayed at any point of time, using the TimeTK package. 

### South East Asia - 11 Countries in 4 rows of 3 countries each

Create Time Series Plots for South East Asia (Brunei, Burma, Cambodia, Timor-Leste, Indonesia, Laos, Malaysia, Philippines, Singapore, Thailand, Vietnam).

Setting .facet_ncol =3 to have 4 rows of 3 countries.

```{r message=FALSE, warning=FALSE}

#selecting only the south east asia countries from the dataset.
SEA <- filter(confirmed_cases_country_level,Country == c("Brunei", "Burma", "Cambodia", "Timor-Leste", "Indonesia", "Laos", "Malaysia", "Philippines","Singapore", "Thailand", "Vietnam")) #11 countries

SEA%>%
   plot_time_series(Date, Daily_new_cases,
                   .facet_ncol =3, .facet_scales = "free",
                   .interactive = TRUE,)


```
### South East Asia - 11 Countries in 6 rows of 2 countries each

Setting .facet_ncol =2 to have 6 rows of 2 countries.


```{r message=FALSE, warning=FALSE}
SEA%>%
   plot_time_series(Date, Daily_new_cases, 
                   .facet_ncol =2, .facet_scales = "free",
                   .interactive = TRUE)
```
### Europe - 15 Countries in 4 rows of 4 countries each

Create Time Series Plots for European Countries (Austria, Belgium, Denmark, Finland,France, Germany, Greece, Ireland, Italy, Luxembourg, Netherlands, Portugal,Spain, Sweden, United Kingdom)

Setting .facet_ncol =4 to have 4 rows of 4 countries.


```{r message=FALSE, warning=FALSE}

#choosing the european countries
EU <- filter(confirmed_cases_country_level,Country == c("Austria", "Belgium", "Denmark", "Finland","France", "Germany", "Greece", "Ireland","Italy", "Luxembourg", "Netherlands", "Portugal","Spain", "Sweden", "United Kingdom"))

EU%>%
   plot_time_series(Date, Daily_new_cases, 
                   .facet_ncol =4, .facet_scales = "free",
                   .interactive = TRUE)

#.plotly_slider = TRUE)
#tk_summary_diagnostics(EU1,Date)
```

### Europe - 15 Countries in 5 rows of 3 countries each

Setting .facet_ncol =3 to have 5 rows of 3 countries.



```{r message=FALSE, warning=FALSE}

EU%>%
   plot_time_series(Date, Daily_new_cases, 
                   .facet_ncol =3, .facet_scales = "free",
                   .interactive = TRUE)

```


### US, China,  - 2 Countries with sliders charts

Setting .plotly_slider = TRUE to try out the slider function.

```{r message=FALSE, warning=FALSE}

Other1 <- filter(confirmed_cases_country_level,Country == c("US","China"))

Other1%>%
   plot_time_series(Date, Daily_new_cases, 
                   .facet_ncol =2, .facet_scales = "free",
                   .interactive = TRUE,
                   .plotly_slider = TRUE)
```

### US  - 1 Country with sliders chart

```{r message=FALSE, warning=FALSE}

US_Confirmed_Cases <- filter(confirmed_cases_country_level,Country == "US")

US_Confirmed_Cases%>%
   plot_time_series(Date, Daily_new_cases, 
                   .facet_ncol =2, .facet_scales = "free",
                   .interactive = TRUE,
                   .plotly_slider = TRUE)
```

### STL Diagnostics - included in TimeTK package - Determining the Trend and Seasonality of the data

The plot_stl_diagnostics generates a Seasonal-Trend-Loess decomposition. Allow the users to view the three components in time series, namely: the trend, seasonality and remainder. 

```{r message=FALSE, warning=FALSE}
#STL Diagnostics

#Using US dataset
US_Confirmed_Cases %>%
    plot_stl_diagnostics(
        Date, Daily_new_cases,
        .frequency = "auto", .trend = "auto",
        .feature_set = c("observed", "season", "trend", "remainder"),
        .interactive = TRUE)

```

### Anomaly Diagnostics - included in TimeTK package - Determining any anomalities in the dataset.

plot_anomaly_diagnostics is an interactive and scalable function for visualizing anomalies in time series data.

```{r message=FALSE, warning=FALSE}

US_Confirmed_Cases %>%
  plot_anomaly_diagnostics(Date, Daily_new_cases, .facet_ncol = 2)

```
### Lag Diagnostics - included in TimeTK package - Determining the Correlation in the dataset

Autocorrelation is the presence of correlation that is connected to lagged versions of a time series. plot_acf_diagnostics() returns the ACF and PACF of a target and optionally CCFâ€™s of one or more lagged predictors in interactive plotly plots.

```{r  message=FALSE, warning=FALSE}
US_Confirmed_Cases %>%
    plot_acf_diagnostics(Date, Daily_new_cases, 
                     .facet_ncol = 2, 
                     .facet_scale = "free",
                     .interactive = TRUE)


```

# EDA - Visualise the daily confirmed cases for each country - Using ggplot package. 


```{r}
ggplot(SEA, aes(Date,Daily_new_cases, color=Country)) +
  geom_path(color='blue', lineend = "round", size=1) +
  facet_wrap(~Country, scales="free") +
  geom_smooth()

```

Trying out the gghighlight() function and removing the smooth line.


```{r}
ggplot(SEA, aes(Date,Daily_new_cases, color=Country)) +
  geom_path(color='blue', lineend = "round", size=1) +
  gghighlight() +
  facet_wrap(~Country, scales="free") 


```
Decomposing time series in Trend, Seasonality and Remainder.

```{r eval = FALSE}

US_Confirmed_Cases %>%
  decompose() %>%
  autoplot()

```

## Obervations for EDA 
Both the TIMETK and GGPLOT packages do have functions to support EDA.

From the visualisation, the following can be observed:

* For most optimal view, to have maximum of 12 countries to be plot (4 rows of 3 countries each) to view the distribution of the data. If not, the chart will be too "squeeze".

* TIMETK package allow interactivity such as PAN, ZOOM etc to be incorporated into the charts by a single argument (.interactive = TRUE). For ggplot, would need to incorporate the use of plotly package to achieve the same effect.

* TIMETK allow Slider to be incorporated by a single arguement (.plotly_slider = TRUE). For ggplot, need to incorporate with plotly package. 

* TIMETK package incorporated function to view the STL and Anomaly and are flexible with the amount of data. For ggplot, the time series need a minimun of 2 periods for the data to be shown.

In conclusion, TIMETK package is more customizable and flexible to be used. Therefore, we will be using the TIMETK package for EDA.



*****************************************************************


# Building Time Series Forecasting Model, using Forecast packages

For a start, the US dataset would be used to build the models.

### Creating the Training and Test dataset

A training set is implemented to build up a model, while a test set is used to validate the model. Usually, data points in the training set are excluded from the test set.

The following code will break the US dataset into Training (80%) and Test dataset (20%).

```{r}
US_train_tbl <- training(initial_time_split(US_Confirmed_Cases, prop = 0.8)) # 80% training and 20% test
US_test_tbl <- testing(initial_time_split(US_Confirmed_Cases, prop = 0.8))

US_train_tbl %>% mutate(type = "train") %>%
  bind_rows(US_test_tbl%>%mutate(type="test"))%>%
  ggplot(aes(x=Date,y=Daily_new_cases,color=type)) +
  geom_line() +  ggtitle("US")

US_train_tbl
US_test_tbl

```

### Fitting the Training data into the respective Models.

```{r message=FALSE, warning=FALSE}

US_tsbl <- as_tsibble(US_train_tbl)

fpp3_model_table <- US_tsbl %>%
  model(
    ## Model 1: Naive ----
    naive_mod = NAIVE(Daily_new_cases),
    ## Model 2: Snaive ----
    snaive_mod = SNAIVE(Daily_new_cases),
    ## Model 3: Drift ----
    drift_mod = RW(Daily_new_cases ~ drift()),
    ## Model 4: SES ----
    ses_mod = ETS(Daily_new_cases ~ error("A") + trend("N") + season("N"), opt_crit = "mse"),
    ## Model 5: Holt's Linear ----
    hl_mod = ETS(Daily_new_cases  ~ error("A") + trend("A") + season("N"), opt_crit = "mse"),
    ## Model 6: Damped Holt's Linear ----
    hldamp_mod = ETS(Daily_new_cases  ~ error("A") + trend("Ad") + season("N"), opt_crit = "mse"),
    ## Model 7: STL decomposition with ETS ----
    stl_ets_mod = decomposition_model(STL(Daily_new_cases), ETS(season_adjust ~ season("N"))),
    ## Model 8: ARIMA ----
    arima_mod = ARIMA(Daily_new_cases),
    ## Model 9: Dynamic harmonic regression ----
    dhr_mod = ARIMA(Daily_new_cases ~ PDQ(0,0,0) + fourier(K=2)),
    ## Model 10: TSLM ----
    tslm_mod = TSLM(Daily_new_cases ~ Date)
  )


```
### Forecast using the Test Data and display the RMSE for each of the model.

```{r }
US_tsbl_test <- as_tsibble(US_test_tbl)

forecast_tbl <- fpp3_model_table %>%
  forecast(US_tsbl_test, times = 0)%>%
   as_tibble() %>%
   select(Date, Daily_new_cases, .model, fc_qty = .mean)



forecast_tbl <- US_Confirmed_Cases %>%
  as_tibble() %>% # change tsibble -> tibble
  select(Date, Daily_new_cases) %>%
  right_join(forecast_tbl, by = c("Date"))%>% # join forecast values
  mutate(fc_qty = ifelse(fc_qty < 0, 0, fc_qty)) # change negative & NA orders



accuracy_tbl <- forecast_tbl %>%
  group_by(.model) %>%
  summarise(accuracy_rmse = Metrics::rmse(Daily_new_cases.x, fc_qty)) # calculate RMSE
  
  accuracy_tbl

```


************************************************************************************

# Building Time Series Forecasting Model, using the ModelTime packages.

For comparison, we will be building the Time Series Model using the ModelTime packages.

Fitting the train data into different model.


```{r message=FALSE, warning=FALSE}
US_train_data <- training(initial_time_split(US_Confirmed_Cases, prop = 0.8)) # 80% training and 20% test
US_test_data <- testing(initial_time_split(US_Confirmed_Cases, prop = 0.8))

US_train_tbl %>% mutate(type = "train") %>%
  bind_rows(US_test_data%>%mutate(type="test"))%>%
  ggplot(aes(x=Date,y=Daily_new_cases,color=type)) +
  geom_line() +  ggtitle("US")

```


```{r message=FALSE, warning=FALSE}

USmodel_fit_arima_no_boost <- arima_reg() %>%
    set_engine(engine = "auto_arima") %>%
    fit(Daily_new_cases ~ Date, data = US_train_data)

USmodel_fit_arima_no_boost



USmodel_fit_arima_boosted <- arima_boost(
    min_n = 2,
    learn_rate = 0.015
) %>%
    set_engine(engine = "auto_arima_xgboost") %>%
    fit(Daily_new_cases ~ Date + as.numeric(Date) + factor(month(Date, label = TRUE), ordered = F),
        data = US_train_data)

USmodel_fit_arima_boosted


USmodel_fit_ets <- exp_smoothing() %>%
    set_engine(engine = "ets") %>%
    fit(Daily_new_cases ~ Date , data = US_train_data)

USmodel_fit_ets




USmodel_fit_prophet <- prophet_reg() %>%
    set_engine(engine = "prophet") %>%
    fit(Daily_new_cases ~ Date, data = US_train_data)

USmodel_fit_prophet


USmodel_fit_lm <- linear_reg() %>%
    set_engine("lm") %>%
    #fit(daily_change ~ as.numeric(Date) + factor(month(Date, label = TRUE), ordered = FALSE),
    fit(Daily_new_cases ~ as.numeric(Date),
        data = US_train_data)

USmodel_fit_lm




USmodel_spec_mars <- mars(mode = "regression") %>%
    set_engine("earth") 

USrecipe_spec <- recipe(Daily_new_cases ~ Date, data = US_train_data) %>%
    step_date(Date, features = "month", ordinal = FALSE) %>%
    step_mutate(date_num = as.numeric(Date)) %>%
    step_normalize(date_num) %>%
    step_rm(Date)
  
USwflw_fit_mars <- workflow() %>%
    add_recipe(USrecipe_spec) %>%
    add_model(USmodel_spec_mars) %>%
    fit(US_train_data)

USwflw_fit_mars



USmodel_snaive <- naive_reg() %>%
    set_engine(engine = "snaive") %>%
    fit(Daily_new_cases ~ Date, data = US_train_data)

USmodel_snaive


USmodel_ETS <- exp_smoothing( 
    error = "additive",
    trend = "additive",
    season = "none",) %>%
    set_engine(engine = "ets") %>%
    fit(Daily_new_cases ~ Date , data = US_train_data)

USmodel_ETS




# adding the models to table

USmodels_tbl <- modeltime_table(
    USmodel_fit_arima_no_boost,
    USmodel_fit_arima_boosted,
    USmodel_fit_ets,
    USmodel_fit_prophet,
    USmodel_fit_lm,
    USwflw_fit_mars,
    USmodel_snaive,
    USmodel_ETS
)

USmodels_tbl


```

## Calibrate the model using test data



```{r message=FALSE, warning=FALSE}
UScalibration_tbl <- USmodels_tbl %>%
    modeltime_calibrate(new_data = US_test_data)
UScalibration_tbl
```




```{r}
UScalibration_tbl %>%
    modeltime_accuracy() %>%
    table_modeltime_accuracy(
        .interactive = TRUE,
        .title = "US Accuracy Table",
        .searchable = FALSE
    )

UScalibration_tbl
```
# Comparing ModelTime and Forecast Package

From the result, it was observed that Forecast and Modeltime packages are able to create similar models (e.g ARIMA, ETS etc). However, it is easier to code using ModelTime package and the results for the models are identical. See image below (highlighted 3 models in which the parameters are aligned and it returned same results.)
![](images/Presentation1.jpg){width=100%}


Upon further investigation, it was observed that ModelTime packages included models from Forecast and Prophet packages. Therefore, ModelTime package would be able to support more models. It also integrated well with TimeTK package. Therefore, i will be using ModelTime package for the model building.
